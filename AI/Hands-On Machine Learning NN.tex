\documentclass[french]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{verbatim}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\ra{1.3}

\title{Hands On Machine Learning ANN}

\begin{document}
\date{}

\maketitle

\section{Training Deep Neural Network}

\subsection{Vanishing / Exploding Gradient}

Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem, which is mostly encountered in recurrent neural networks

Xavier Glorot and Yoshua Bengio found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time, namely random initialization using a normal distribution with a mean of 0 and a standard deviation of 1. 

the connection weights of each layer must be initialized randomly as: \\

\begin{tabular}{@{}lll@{}}
    \toprule
    Initialization & Activation functions & $\sigma^2$ (Normal distribution) \\
    \hline
    Glorot & None, tanh, Logistic, Softmax & $1/fan_{avg}$ \\
    He & ReLU \& variants & $2/fan_{in}$ \\
    LeCun & SELU & $1/fan_{in}$ \\
    \bottomrule
\end{tabular}
$ $ \\

For a uniform distribution (instead of normal) between $-r$ and $+r$, juste compute $r = \sqrt(3 \sigma^2)$


$fan_{in}$ and $fan_{out}$ are the number of inputs and the number of neurons

\subsection{Nonsaturating Activation Functions}

One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing / exploding gradients problems were in part due to a poor choice of activation function. ReLU solves some of the problem. Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs. Tol solve this problem, you can use one of the ReLU variants:

\begin{description}
    \item[ReLU] $ReLU(z) = max(0, z)$
    \item[Leaky ReLU] $LeakyReLU_\alpha(z) = max(\alpha z, z)$\\
        This hyperparameter $\alpha$ defines how much the function "leaks". It's the slope of the function for $z < 0$ and typically set to $0.01$. This ensures the function never dies. LeakyReLU usually outperforms ReLU.
    \item[Randomized Leaky ReLU] $\alpha$ is picked randomly in a given range during training, and fixed to an average value during testing. Perfoms well and seems to act as a regularizer.
    \item[Parametric leaky ReLU] $\alpha$ is learned during training. Instead of a hyperparameter, it becomes a parameter. This strongly outperformed ReLU on large images datasets but runs the risk of overfitting on smaller datasets.
    \item[Exponential Linear Unit (ELU)]: Outperformed all the ReLU variants (training time reduced and better performance on the test set). The main drawback is that is that it's slower to compute, but during training it's compensated by faster convergence rate. However at test time, an ELU network will be slower than a ReLU network.
      \[   
      ELU_{\alpha}(z) =
           \begin{cases}
             \alpha (\exp(z) - 1)  &\quad\text{if } z < 0\\
             z &\quad\text{if } z \geq 0 \\
           \end{cases}
      \]
    \item[Scaled Exponential Linear Unit (SELU)]: Scaled version of ELU. Great performance when network is composed exclusively of a stack of dense layers.
\end{description}

In general SELU > ELU > Leaky ReLU (and variants) > ReLU > tanh > logistic

\subsection{Batch Normalization}

A well chosen initialization (plus activation function) can significantly reduce the vanishing / exploding gradients problems at the beginning of training but doesn't guarantee that they won't come back during training.

Batch Normalization is a technique that can be used to address the problem. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer, simply zero-centering and normalizing each input, then scaling and shifting the result using two new parameter vectors per layer: one for scaling, the other for shifting.

\begin{enumerate}
    \item $\bm{\mu_B} = \frac{1}{m_B} \sum_{i=1}^{m_B} \bm{x^{(i)}}$
    \item $\bm{\sigma_B^2} = \frac{1}{m_B} \sum_{i=1}^{m_B} (\bm{x^{(i)}} - \bm{\mu_B})^2$
    \item $\bm{\hat x^{(i)}} = \frac{\bm{x^{(i)}} - \bm{\mu_B}}{\sqrt{\bm{\sigma_B^2} + \epsilon}}$
    \item $\bm{z^{(i)}} = \bm{\gamma} \otimes \bm{\hat x^{(i)}} + \bm{\beta}$
\end{enumerate}

\begin{itemize}
    \item $\bm{\mu_B}$ vector of input means, evaluated over the whole mini-batch
    \item $\bm{\sigma_B^2}$ vector of input standard deviations, also evaluated over the whole minibatch
    \item $\bm{\gamma}$ output scale parameter vector for the layer (each input is multiplied by its corresponding output scale parameter)
    \item $\bm{\beta}$ output shift parameter vector for the layer
    \item $\epsilon$ to avoid division by zero
\end{itemize}

$ $

$\gamma$ and $\beta$ are learned through regular backpropagation, and $\mu$ and $\sigma$ are estimated during training (but not used during training). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function.  The networks were also much less sensitive to the weight initialization. Finally, like a gift that keeps on giving, Batch Normalization also acts like a regularizer.

\subsection{Gradient Clipping}

Another popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNN. For other types of networks, BN is usually sufficient.

It's possible to clip by value(may change the orientation of the gradient vector) or by norm.

\subsection{Reusing Pretrained Layers}

\subsection{Faster Oprimizers}

\begin{description}
    \item[Momentum Optimization: ] It will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity:
        \begin{enumerate}
            \item $\bm{m} \leftarrow \beta \bm{m} - \eta \triangledown_{\bm{\theta}} J(\bm{\theta})$
            \item $\bm{\theta} \leftarrow \bm{\theta} + \bm{m}$
        \end{enumerate}
Momentum optimization allows to escape from plateaus much faster than Gradient Descent. The one drawback of Momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent.
    \item[Nesterov Accelerated Gradient: ] One small variant to Momentum optimization almost always faster than vanilla Momentum optimization. The idea of Nesterov
Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum:
        \begin{enumerate}
            \item $\bm{m} \leftarrow \beta \bm{m} - \eta \triangledown_{\bm{\theta}} J(\bm{\theta + \beta \bm{m}})$
            \item $\bm{\theta} \leftarrow \bm{\theta} + \bm{m}$
        \end{enumerate}
    \item[AdaGrad: ] Scalles down the gradient vector along the steepest dimensions. In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate.  It helps point the resulting updates more directly toward the global optimum:
        \begin{enumerate}
            \item $\bm{s} \leftarrow \bm{s} + \triangledown_{\bm{\theta}} J(\bm{\theta}) \otimes \triangledown_{\bm{\theta}} J(\bm{\theta})$
            \item $\bm{\theta} \leftarrow \bm{\theta} - \eta \triangledown_{\bm{\theta}} J(\bm{\theta}) \oslash \sqrt{\bm{s} + \epsilon}$
        \end{enumerate}
AdaGrad often performs well for simple quadratic problems, but unfortunately it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum.
    \item[RMSProp: ] Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm 15 fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training).
        \begin{enumerate}
            \item $\bm{s} \leftarrow \beta \bm{s} + (1 - \beta) \triangledown_{\bm{\theta}} J(\bm{\theta}) \otimes \triangledown_{\bm{\theta}} J(\bm{\theta})$
            \item $\bm{\theta} \leftarrow \bm{\theta} - \eta \triangledown_{\bm{\theta}} J(\bm{\theta}) \oslash \sqrt{\bm{s} + \epsilon}$
        \end{enumerate}
    \item[Adam and Nadam Optimization: ] Adaptive moment estimation, combines the ideas of Momentum optimization and RMSProp
\end{description}

The optimization literature contains amazing algorithms based on the second-order partial derivatives (the Hessians, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are $n^2$ Hessians per output (where n is the
number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often donâ€™t even fit in memory, and even when they do, computing the Hessians is just too slow.

\subsection{Learning Rate Scheduling $\eta$}

If learning rate is set way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time. As discussed previously, one approach is to start with a large learning rate, and divide it by 3 until the training algorithm stops diverging.

However, you can do better than a constant learning rate: start with a high learning rate and reduce it once it stops making fast progress. These strategies are called learning schedules:

\begin{description}
    \item[Power scheduling] function of the iteration number $t$: $\eta(t) = \eta_0 / (1+ t/s)^c$ \\
      This schedule first drops quickly, then more and more slowly. Of course, this requires tuning $\eta_0$ , $s$ (and possibly $c$ usually set to 1)
    \item[Exponential scheduling] $\eta(t) = \eta_0 0.1^{t/s}$ \\
    The learning rate will gradually drop by a factor of 10 every $s$ steps
    \item[Piecewise constant scheduling] Use a constant learning rate for a number of epochs. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates, and how long to use each of them
    \item[Performance scheduling] Measure the validation error every $N$ steps (just like for early stopping) and reduce the learning rate by a factor of $\lambda$ when the error stops dropping.
\end{description}


\end{document}
