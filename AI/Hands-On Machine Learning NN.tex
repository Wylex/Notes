\documentclass[french]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{verbatim}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\ra{1.3}

\title{Hands On Machine Learning ANN}

\begin{document}
\date{}

\maketitle

\section{Training Deep Neural Network}

\subsection{Vanishing / Exploding Gradient}

Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem, which is mostly encountered in recurrent neural networks

Xavier Glorot and Yoshua Bengio found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time, namely random initialization using a normal distribution with a mean of 0 and a standard deviation of 1. 

the connection weights of each layer must be initialized randomly as: \\

\begin{tabular}{@{}lll@{}}
    \toprule
    Initialization & Activation functions & $\sigma^2$ (Normal distribution) \\
    \hline
    Glorot & None, tanh, Logistic, Softmax & $1/fan_{avg}$ \\
    He & ReLU \& variants & $2/fan_{in}$ \\
    LeCun & SELU & $1/fan_{in}$ \\
    \bottomrule
\end{tabular}
$ $ \\

For a uniform distribution (instead of normal) between $-r$ and $+r$, juste compute $r = \sqrt(3 \sigma^2)$


$fan_{in}$ and $fan_{out}$ are the number of inputs and the number of neurons

\subsection{Nonsaturating Activation Functions}

One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing / exploding gradients problems were in part due to a poor choice of activation function. ReLU solves some of the problem. Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs. Tol solve this problem, you can use one of the ReLU variants:

\begin{description}
    \item[ReLU] $ReLU(z) = max(0, z)$
    \item[Leaky ReLU] $LeakyReLU_\alpha(z) = max(\alpha z, z)$\\
        This hyperparameter $\alpha$ defines how much the function "leaks". It's the slope of the function for $z < 0$ and typically set to $0.01$. This ensures the function never dies. LeakyReLU usually outperforms ReLU.
    \item[Randomized Leaky ReLU] $\alpha$ is picked randomly in a given range during training, and fixed to an average value during testing. Perfoms well and seems to act as a regularizer.
    \item[Parametric leaky ReLU] $\alpha$ is learned during training. Instead of a hyperparameter, it becomes a parameter. This strongly outperformed ReLU on large images datasets but runs the risk of overfitting on smaller datasets.
    \item[Exponential Linear Unit (ELU)]: Outperformed all the ReLU variants (training time reduced and better performance on the test set). The main drawback is that is that it's slower to compute, but during training it's compensated by faster convergence rate. However at test time, an ELU network will be slower than a ReLU network.
      \[   
      ELU_{\alpha}(z) =
           \begin{cases}
             \alpha (\exp(z) - 1)  &\quad\text{if } z < 0\\
             z &\quad\text{if } z \geq 0 \\
           \end{cases}
      \]
    \item[Scaled Exponential Linear Unit (SELU)]: Scaled version of ELU. Great performance when network is composed exclusively of a stack of dense layers.
\end{description}

In general SELU > ELU > Leaky ReLU (and variants) > ReLU > tanh > logistic

\subsection{Batch Normalization}

A well chosen initialization (plus activation function) can significantly reduce the vanishing / exploding gradients problems at the beginning of training but doesn't guarantee that they won't come back during training.

Batch Normalization is a technique that can be used to address the problem. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer, simply zero-centering and normalizing each input, then scaling and shifting the result using two new parameter vectors per layer: one for scaling, the other for shifting.

\begin{enumerate}
    \item $\bm{\mu_B} = \frac{1}{m_B} \sum_{i=1}^{m_B} \bm{x^{(i)}}$
    \item $\bm{\sigma_B^2} = \frac{1}{m_B} \sum_{i=1}^{m_B} (\bm{x^{(i)}} - \bm{\mu_B})^2$
    \item $\bm{\hat x^{(i)}} = \frac{\bm{x^{(i)}} - \bm{\mu_B}}{\sqrt{\bm{\sigma_B^2} + \epsilon}}$
    \item $\bm{z^{(i)}} = \bm{\gamma} \otimes \bm{\hat x^{(i)}} + \bm{\beta}$
\end{enumerate}

\begin{itemize}
    \item $\bm{\mu_B}$ vector of input means, evaluated over the whole mini-batch
    \item $\bm{\sigma_B^2}$ vector of input standard deviations, also evaluated over the whole minibatch
    \item $\bm{\gamma}$ output scale parameter vector for the layer (each input is multiplied by its corresponding output scale parameter)
    \item $\bm{\beta}$ output shift parameter vector for the layer
    \item $\epsilon$ to avoid division by zero
\end{itemize}

$\gamma$ and $\beta$ are learned through regular backpropagation, and $\mu$ and $\sigma$ are estimated during training (but not used during training). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function.  The networks were also much less sensitive to the weight initialization. Finally, like a gift that keeps on giving, Batch Normalization also acts like a regularizer.

\subsection{Gradient Clipping}


\end{document}
