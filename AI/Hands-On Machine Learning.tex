\documentclass[french]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{verbatim}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\ra{1.3}

\title{Hands-On Machine Learning}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  frame=leftline,
  rulecolor=\color{gray},
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a }}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}

\begin{document}
\tableofcontents
\date{}

\maketitle

\setlength{\parindent}{0cm}

\section{Remarks}

\subsection{Scikit-learn Design}

$\rightarrow$ Estimators: object that can estimate some parameters. Implements the \lstinline{fit} method
\begin{itemize}
    \item[.] All the hyperparameters are accesible via public instance variables
    \item[.] All the learned parameters are accesible via public instance variables with an underscore suffix
\end{itemize}
$\rightarrow$ Transformers: some estimators can transform a dataset. Implements the \lstinline{transform} method\\
$\rightarrow$ Predictors: some estimators are capable of making predictions. Implements the \lstinline{predict} and \lstinline{score} methods

\subsection{Select a Performance Measure}

\subsubsection{Regression}

\begin{itemize}

\item[-] A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes, with a higher weitht for large errors:

\[RMSE(\bm{X}, h) = \sqrt{\frac{1}{m} \sum_{i=1}^m(h(\bm{x}^{(i)}) - y^{(i)})^2}\]

\item[-] Even though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, suppose there are many outliers. In that case you may consider using the Mean Absolute Error:

\[MAE(\bm{X}, h) = \frac{1}{m} \sum_{i=1}^m |h(\bm{x}^{(i)}) - y^{(i)}|\]

\end{itemize}


\section{End-to-End Machine Learning Project}

\begin{enumerate}

\item Quick understanding of the data
    \begin{itemize}
        \item[-] General info
        \item[-] Visualize, plot
    \end{itemize}
\item Create the Test Set
    \begin{itemize}
        \item[-] Random split
        \item[-] Stratified sampling: \\
             $\rightarrow$ keep same proportions for a given feature
    \end{itemize}
\item Discover and Visualize
    \begin{itemize}
        \item[-] Put the test set aside
        \item[-] Scatter plot: \\
             $\rightarrow$ Any 2D data (p.e. geographical data) \\
             $\rightarrow$ Play with dot size, color and alpha value
        \item[-] Correlations (linear)
        \item[-] Experimenting with Attribute Combinations
    \end{itemize}
\item Data cleaning
    \begin{itemize}
        \item[-] Missing values (most ML algorithms cannot work with missing features):\\
             $\rightarrow$ Get rid of the instance \\
             $\rightarrow$ Get rid of the feature \\
             $\rightarrow$ Set the values to something (zero, mean, median...)
        \item[-] Handle text and categorical attributes \\
             $\rightarrow$ Most ML algorithms can only work with numbers \\
             $\rightarrow$ Convert categories from text to numbers (OrdinalEncoder, OneHotEncoder...)
        \item[-] Create custom transformers
        \item[-] Feature Scaling \\
            $\rightarrow$ min-max scaling \\
            $\rightarrow$ standardization: Much less affected by outliers
        \item[-] Pipelines: \\
            $\rightarrow$ All but the last estimator must be transformers
    \end{itemize}
\item Select and train a Model
    \begin{itemize}
        \item[-] Test a few models
        \item[-] Fine-Tune yout model \\
            $\rightarrow$ Gridsearch: it will evaluate every possible combination of hyperparameters values \\
            $\rightarrow$ Randomized Search
    \end{itemize}
\item Evaluate on Test Set
\end{enumerate}


\begin{comment}
\section{End-to-End Machine Learning Project}

\subsection{Extract the data}

Get a quick description of the data (number of rows and each attribute's type):
\begin{lstlisting}
  df.info()
\end{lstlisting}

Find out what categories exist:
\begin{lstlisting}
  df['c'].value_counts()
\end{lstlisting}

Summary of the numerical attributes:
\begin{lstlisting}
  df.describe()
\end{lstlisting}

Plot histogram for each numerical attribute:
\begin{lstlisting}
  %matplotlib inline
  import matplotlib.pyplot as plt
  df.hist(bins=50)
  plt.show()
\end{lstlisting}
Rq. Calling \verb|show()| in a Jupyter notebook is optional


\subsection{Test set}

Create a test set:
\begin{lstlisting}
  def split_train_set(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffle_indices[test_set_size:]

    return data.iloc[train_indices], data.iloc[test_indices]
\end{lstlisting}

Rq. If the program runs again, the test set will not be the same which is a problem.\\
One solution is to save the test set. Another option is to set the random number generator's seed. Both solutions will break next time you fetch an updated dataset. A common solution is to use each instance's identifier (unique and immutable) to decide whether or not it should go in the test set. For example, you could compute a hash of each instance's identifier and put that instance in the test set if the hash is lower or equal to 20\%
\begin{lstlisting}
  from zlib import crc32

  def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

  def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))

    return data.loc[~in_test_set], data.loc[in_test_set]
\end{lstlisting}

Rq. If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset, and no row ever gets deleted. If this is not possible, then you can try to use the most stable featues to build a unique identifier.\\

Scikit-learn provides a few function to split datasets. \\The simplest is \verb|split_train_test|, which does pretty much the same thing as the one defined earlier with a couple additional features. First there is a \verb|random_state| parameter that allows you to set the random generator seed and second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (useful if you have a separate DataFrame for labels)


\begin{lstlisting}
  from sklearn.model_selection import train_set_split

  train_set, test_set = train_set_split(data, test_size=0.2, random_state=42)
\end{lstlisting}

There are other kind of splits. For example there's the stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population.

\begin{lstlisting}
  from sklearn.model_selection import StratifiedShuffleSplit

  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  for train_index, test_index in split.split(df, df['categorized_feature']):
    strat_train_set = df.loc[train_index]
    strat_test_set = df.loc[test_index]
\end{lstlisting}

This split will make sure to keep the same proportions of the categories in \\\lstinline{df['categorized_feature']} in the test set as in the full set.\\

Rq. It can be interesting to use this kind of split even if the feature is not categorized. For example for house price prediction, median income could be an important feature. Since it's continuous you need to create a categorical attribute. For that you can visualize the histogram and create a new feature with \lstinline{pd.cut} that you can remove once the split is done.

\subsection{Discover the Data}

\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude")
\end{lstlisting}

Visualize density:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
\end{lstlisting}

you may need to play around with visualization parameters to make the patterns stand out.\\

You can include the district's population and the prices:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", figsize=(10,7), c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True)
  plt.legend()
\end{lstlisting}

\subsection{Looking for Correlations}

\begin{lstlisting}
  corr_matrix = df.corr()
  corr_matrix["median_house_value"].sort_values(ascending=False)
\end{lstlisting}

Rq. The correlation coefficient only measures linear correlations. It may completely miss out on nonlinear relationships.\\

Once you have a few promising attributes, you can plot them against each other with Pandas:
\begin{lstlisting}
  from pandas.plotting import scatter_matrix

  attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

  scatter_matrix(housing[attributes], figsize=(12, 8))
\end{lstlisting}

\subsection{Data Cleaning}

Most Machine Learning algorithms cannot work with missing features. You have three options:
\begin{itemize}
  \item [-] Get rid of the correspondig entries
  \item [-] Get rid of the whole attribute
  \item [-] Set the values to some value (zero, the mean, the median...)
\end{itemize}

You can accomplish these easily using \lstinline{dropna(), drop(), fillna()}
\begin{lstlisting}
  df.dropna(subset=["column"])
  df.drop("column", axis=1)
  median = df["column"].median()
  df["column"].fillna(median, inplace=True)
\end{lstlisting}

Scikit-Learn provides a handy class to take care of missing values: \lstinline{SimpleImputer}
\begin{lstlisting}
  from sklearn.impute import SimpleImputer

  imputer = SimpleImputer(strategy="median")
\end{lstlisting}

You need do drop any non numeric attributes and then fit the imputer instance to the training data:
\begin{lstlisting}
  imputer.fit(df)
\end{lstlisting}

Now you can use this ``trained'' imputer to transform the training set:
\begin{lstlisting}
  X = imputer.transform(df)
\end{lstlisting}

The result is a NumPy array that you can put back into a Pandas DataFrame:
\begin{lstlisting}
  df_transformed = pd.DataFrame(X, columns=df.columns)
\end{lstlisting}

\subsection{Handling Text and Categorical Attributes}

Most Machine Learning algorithms prefer to work with numbers. It's possible to convert categories from text to numbers. For this, we can use Scikit-Learn's OrdinalEncoder class:
\begin{lstlisting}
  from sklearn.preprocessing import OrdinalEncoder

  ordinal_encoder = OrdinalEncoder()
  df_encoded = ordinal_encoder.fit_transform(df)
\end{lstlisting}

One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g. for ordered categories such as ``bad'', ``average'' and ``good''). One solution is to use one-hot encoding:
\begin{lstlisting}
  from sklearn.preprocessing import OneHotEncoder

  encoder = OneHotEncoder()
  df_1hot = encoder.fit_transform(df)
\end{lstlisting}

Notice that the output is a SciPy sparse matrix. This is very useful when you have categorical attributes with thousands of categories.

\section{Scikit-learn Design}

All objects share a consistent and simple interface:
\begin{description}
  \item [Estimators] Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the \lstinline{fit()} method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an imputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter).
  \item [Transformers] Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the \lstinline{transform()} method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called \lstinline{fit_transform()} that is equivalent to calling \lstinline{fit()} and then \lstinline{transform()} (but sometimes \lstinline{fit_transform()} is optimized and runs much faster).
  \item [Predictors] Finally, some estimators are capable of making predictions given a dataset; they are called predictors. For example, the LinearRegression model in the previous chapter was a predictor: it predicted life satisfaction given a country’s GDP per capita. A predictor has a \lstinline{predict()} method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a \lstinline{score()} method that measures the quality of the predictions given a test set (and the corresponding labels in the case of supervised learning
algorithms)
\end{description}


\section{Custom transformers}

All you need is to create a class and implement three methods: \lstinline{fit(), self(), fit_tranform()}\\

You can get the last one for free by symply adding \lstinline{TransformerMixin} as base class. Also if you add \lstinline{BaseEstimator} as a base class you will get two extra methods: \lstinline{get_params(), set_params()}

\subsection{Feature scaling}

With few exceptions, ML algorithms don't perform well when the input numerical attributes have very different scales.\\

There are two common ways to get all attributes to have the same scale.  min-max scaling and standardization. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms. However, standardization is much less affected by outliers.

\subsection{Transformation pipelines}

There are many data transformation steps that need to be executed in the right order. Scikit-learn provides the \lstinline{Pipeline} class to help with such sequences of transformations.

The \lstinline{Pipeline} constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e have the \lstinline{fit_transform} method)

\begin{lstlisting}
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
('imputer', SimpleImputer(strategy="median")),
('attribs_adder', CombinedAttributesAdder()),
('std_scaler', StandardScaler()),
])

housing_num_tr = num_pipeline.fit_transform(housing_num)
\end{lstlisting}

The pipeline exposes the same methods as the final estimator.\\

It's possible to create a pipeline for both numerical and categorical attributes:

\begin{lstlisting}
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num) # numerical columns
cat_attribs = ["ocean_proximity"] # categoric columns

full_pipeline = ColumnTransformer([
("num", num_pipeline, num_attribs),
("cat", OneHotEncoder(), cat_attribs),
])

housing_prepared = full_pipeline.fit_transform(housing)
\end{lstlisting}

Rq. By default the remaining columns (the ones not listed) will be dropped

\section{Select and Train a Model}

\subsection{Evaluate a model}

You don't want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part for model validation.

You can use Scikit-Learn's \lstinline{K-fold cross-validation} feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result in an array containing the 10 evaluation scores.

\begin{lstlisting}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)

tree_rmse_scores = np.sqrt(-scores)
\end{lstlisting}

Rq. Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root

\section{Fine tune your model}

Let's assume you now have a short list of promising models. You now need to fine-tune them.

\subsection{Grid Search}

All you need to do is tell \lstinline{GridSearchCV} which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameters values, using cross-validation.

\subsection{Randomized Search}

The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use \lstinline{RandomizedSearchCV} instead. Instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.

\subsection{Ensemble Methods}

Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors.


\subsection{Evaluate your system on the test set}
After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your \lstinline{full_pipeline} to transform the data (call \lstinline{transform()} , not \lstinline{fit_transform()} , you do not want to fit the test set!), and evaluate the final model on the test set

\begin{lstlisting}
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
\end{lstlisting}

You might want to have an idea of how precise this estimate is.  For this, you can compute a 95\% confidence interval for the generalization error using scipy.stats.t.interval() :
\begin{lstlisting}
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))
\end{lstlisting}

\end{comment}

\section{Classification}

\subsection{Performance measures}

Evaluating a classifier is often significantly trickier than evaluating a regressor. Accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).

\subsubsection{Confusion Matrix}

The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5\textsuperscript{th} row and 3\textsuperscript{rd} column of the confusion matrix.\\

The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the precision of the classifier.

\[precision = \frac{TP}{TP + FP}\]

Precision is typically used along with another metric named recall, also called sensitivity or true positive rate.

\[recall = \frac{TP}{TP + FN}\]

It is often convenient to combine precision and recall into a single metric called the $F_1$ score, in particular if you need a simple way to compare two classifiers. The $F_1$ score is the harmonic mean of precision and recall.

Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.  As a result, the classifier will only get a high $F_1$ score if both recall and precision are high.

\[F_1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}}\]

The F\textsubscript{1} score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.

\subsubsection{Precision/Recall Tradeoff}

Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s \lstinline{predict()} method, you can call its \lstinline{decision_function()} method, which returns a score for each instance, and then make predictions based on those scores using any threshold you want.

\subsubsection{The ROC Curve}

The Receiver Operating Characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive.\\

Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise.

\subsection{Multiclass Classification}

Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers.

\begin{enumerate}
    \item For example, one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest).
    \item nother strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy.
\end{enumerate}

Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO)

\subsection{Multilabel Classification}

Until now each instance has always been assigned to just one class. In some cases you may want your classifier to output multiple classes for each instance.

\subsection{Multioutput Classification}

It is simply a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).

\section{Training Models}

\subsection{Normal Equation (Linear Regression)}

A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term.
\[\hat y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n = h_{\bm{\theta}}(\bm{x}) = \bm{\theta^T} \bm{x}\]
Training a model means setting its parameters so that the model best fits the training set. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). It is simpler to minimize the Mean Square Error (MSE) Linear Regression than the RMSE, and it leads to the same result.
\[ MSE(\bm{X}, h_{\bm{\theta}}) = \frac{1}{m} \sum_{i=1}^m (\bm{\theta}^T\bm{x}^{(i)} - \bm{y}^{(i)}  )^2\]

\subsubsection{The Normal Equation}
To find the value of $\bm{\theta}$ that minimizes the cost function, there is a closed-form solution in other words, a mathematical equation that gives the result directly. This is called the Normal Equation:
\[\hat \theta = (\bm{X}^T \bm{X})^{-1}\bm{X}^T \bm{y}\]

\subsubsection{SVD}

The Normal Equation can be approximated using the pseudoinverse which is computed using a standard matrix factorization technique called Singular Value Decomposition (SVD).

\subsection{Gradient Descent}

Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. It measures the local gradient of the error function with regards to the parameter vector $\theta$, and it goes in the direction of descending gradient.\\

Concretely, you start by filling $\bm{\theta}$ with random values (this is called random initialization), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum. \\

An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter.

\subsubsection{Batch Gradient Descent}

To implement Gradient Descent, you need to compute the gradient of the cost function with regards to each model parameter $\theta_j$.\\

Instead of computing these partial derivatives individually, you can use The gradient vector, noted $\nabla_{\bm{\theta}}MSE(\bm{\theta})$ which contains all the
partial derivatives of the cost function (one for each model parameter).

\[\nabla_{\bm{\theta}}MSE(\bm{\theta}) = \frac{2}{m} \bm{X}^T(\bm{X \theta - y}) \]
Notice that this formula involves calculations over the full training set $\bm{X}$, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name).

As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features.\\

Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\nabla_{\bm{\theta}}MSE(\bm{\theta})$ from $\bm{\theta}$. This is where the learning rate $\eta$ comes into play: multiply the gradient vector by $\eta$ to determine the size of the downhill step.
\[\bm{\theta}^{(next\ step)} = \bm{\theta} - \eta \nabla_{\bm{\theta}}MSE(\bm{\theta})\]

\begin{lstlisting}
eta = 0.1
n_iterations = 1000
m = 100

theta = np.random.randn(2,1)

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
\end{lstlisting}

The result is pretty much the same as with the Normal Equation!

\subsubsection{Stochastic Gradient Descent}

The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance.\\

This algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down

\begin{lstlisting}
n_epochs = 50
t_0, t_1 = 5, 50

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for i in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
\end{lstlisting}

\subsubsection{Mini-batch Gradient Descent}

At each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Minibatch GD computes the gradients on small random sets of instances called minibatches.\\

\subsubsection{Comparison of algorithms for Linear Regression}

\begin{tabular}{@{}llllll@{}}
    \toprule
    Algorithm & Large m & Large n & Hyperparams & Scalling required & Scikit-Learn \\
    \hline
    Normal Equation & Fast & Slow & 0 & No & n/a \\
    SVD & Fast & Slow & 0 & No & LinearRegression \\
    Batch GD & Slow & Fast & 2 & Yes & SGDRegressor \\
    Stochastic GD & Fast & Fast & $\geq 2$ & Yes & SGDRegressor \\
    Mini-batch GD & Fast & Fast & $\geq 2$ & Yes & SGDRegressor \\
    \bottomrule
\end{tabular}


\subsection{Learning Curves}

How can you tell that your model is overfitting or underfitting the data?

You can use cross-validation to get an estimate of a model’s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.\\

Another way is to look at the learning curves: these are plots of the model’s performance on the training set and the validation set as a function of the training set size (or the training iteration).\\

If both curves have reached a plateau, they are close and fairly high, these learning curves are typical of an underfitting model. If your model is underfitting the training data, adding more training examples will not help. You need to use a more complex model or come up with better features.\\

If however there is a gap between the curves, this means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer. One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error.

\subsubsection{The Bias / Variance Tradeoff}

An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors:

\begin{description}
    \item [Bias]: This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.
    \item [Variance]: This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.
    \item [Irreducible Error]: This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).
\end{description}

Increasing a model’s complexity will typically increase its variance and reduce its bias.  Conversely, reducing a model’s complexity increases its bias and reduces its variance.  This is why it is called a tradeoff.

\subsection{Regularized Linear Models}

A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees.\\

For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.

\subsubsection{Ridge Regression}

A regularization term equal to $\alpha \sum_{i=1}^n \theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible.\\

Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.

It is quite common for the cost function used during training to be different from the performance measure used for testing. Apart from regularization, another reason why they might be different is that a good training cost function should have optimization-friendly derivatives, while the performance measure used for testing should be as close as possible to the final objective.\\

The hyperparameter $\alpha$ controls how much you want to regularize the model. If $\alpha$ = 0 then Ridge Regression is just Linear Regression. If $\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean.

Ridge Regression cost function:
\[J(\bm{\theta}) = MSE(\bm{\theta}) + \alpha \sum_{i=1}^n \theta_i^2\]
Note that the bias term $\theta_0$ is not regularized.\\

As with Linear Regression, we can perform Ridge Regression either by computing a closed-form equation or by performing Gradient Descent. The pros and cons are the same.

\subsubsection{Lasso Regression}

Just like Ridge Regression, it adds a regularization term to the cost function, but it uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm.

Lasso Regression cost function:
\[J(\bm{\theta}) = MSE(\bm{\theta}) + \alpha \sum_{i=1}^n |\theta_i|\]

An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).

\subsubsection{Elastic Net}

Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression.

Elastic Net cost function:
\[J(\bm{\theta}) = MSE(\bm{\theta}) + r \alpha \sum_{i=1}^n |\theta_i| + \frac{1-r}{2} \alpha \sum_{i=1}^n \theta_i^2\]

So when should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.

\subsubsection{Early Stopping}

A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping.

\section{Regression}

\subsection{Polynomial Regression}

What if your data is actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.

\subsection{Logistic Regression}

Some regression algorithms can be used for classification as well (and vice versa). Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class.\\

So how does it work? Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the logistic of this result.\\

\[h_{\bm{\theta}}(\bm{x}) = \sigma (\bm{x}^T \bm{\theta})\]

The logistic noted $\sigma(.)$ is a sigmoid function that outputs a number between 0 and 1.
\[\sigma(t) = \frac{1}{1+\exp{(-t)}}\]

Once the Logistic Regression model has estimated the probability $\hat p = h_{\theta}(\bm{x})$ that an instance belongs to the positive class, it can make its prediction $\hat y$ easily.

\subsubsection{Training and Cost Function}
\[
    c(\bm{\theta}) =
    \begin{cases}
        -\log{(\hat p)} & \text{if $y=1$}\\
        -\log{(1 - \hat p)} & \text{if $y=0$}
    \end{cases}
\]
The cost function over the whole training set is simply the average cost over all training instances.\\
Logistic Regression cost function:
\[J(\bm{\theta}) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)}\log(\hat p^{(i)}) + (1-y^{(i)})\log(1- \hat p^{(i)})]\]

The bad news is that there is no known closed-form equation to compute the value of $\bm{\theta}$ that minimizes this cost function (there is no equivalent of the Normal Equation).  But the good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough).

\subsection{Softmax Regression}

The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.\\

The idea is quite simple: when given an instance $\bm{x}$, the Softmax Regression model first computes a score $s_k(\bm{x})$ for each class k, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores.\\

Note that each class has its own dedicated parameter vector $\bm{\theta}(k)$. All these vectors are typically stored as rows in a parameter matrix $\bm{\Theta}$.

Cross entropy cost function:
\[J(\bm{\Theta}) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log(\hat p_k^{(i)})\]

\section{Support Vector Machines}

A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. SVMs are particularly well suited for classification of complex but small or medium-sized datasets.\\

You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes.  This is called large margin classification. Adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors.

\subsection{Linear SVM Classification}

\subsubsection{Soft Margin Classification}

If we strictly impose that all instances be off the street and on the right side, this is called hard margin classification. There are two main issues with hard margin classification. First, it only works if the data is linearly separable, and second it is quite sensitive to outliers.\\

To avoid these issues it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations. This is called soft margin classification.

\subsection{Nonlinear SVM Classification}

Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features.

\subsubsection{Polynomial Kernel}

Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow. \\

Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the kernel trick (it is explained in a moment). It makes it possible to get the same result as if you added many polynomial features, even with very high degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features since you don’t actually add any features. This trick is implemented by the SVC class.

\subsubsection{Gaussian RBF Kernel}

Another technique to tackle nonlinear problems is to add features computed using a similarity function that measures how much each instance resembles a particular landmark. \\

Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them.

\subsection{SVM Regression}

As we mentioned earlier, the SVM algorithm is quite versatile: not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression. The trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations.

\section{Decision Trees}

Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require feature scaling or centering at all.

\subsection{Instability}
Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis) which makes them sensitive to training set rotation. More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data.

\section{Ensemble Learning and Random Forests}

If you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble.

You will often use Ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in Machine Learning competitions often involve several Ensemble methods.

\subsection{Bagging and Pasting}

One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set.
When sampling is performed with replacement, this method is called bagging. When sampling is performed without replacement, it is called pasting.

\subsubsection{Random Forests}

Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max samples set to the size of the training set.

\subsection{Boosting}

Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor.

\subsubsection{AdaBoost}

One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.

\subsubsection{Gradient Boosting}

Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.

\subsection{Stacking}

It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation?

\section{Dimensionality Reduction}

Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality), so even though it will speed up training, it may also make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train your system with the original data before considering using dimensionality reduction if training is too slow.

In some cases, however, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance (but in general it won’t; it will just speed up training).\\

Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization (or DataViz). Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters.

Moreover, DataViz is essential to communicate your conclusions to people who are not data scientists, in particular decision makers who will use your results.

\subsection{The Curse of Dimensionalilty}

High dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations.  In short, the more dimensions the training set has, the greater the risk of overfitting it.\\

In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions.

\subsection{Main Approches for Dimensionality Reduction}

\subsubsection{Projection}

In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated.  As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.

\subsubsection{Manifold Learning}

The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension. \\

Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed.\\

The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. However, this assumption does not always hold.

In short, if you reduce the dimensionality of your training set before training a model, it will usually speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset.

\subsection{Dimensionality Reduction Algorithms}

\subsubsection{PCA}

Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\\

It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA.\\

So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix $\bm{X}$ into the matrix multiplication of three matrices $\bm{U\ \Sigma\ V^T}$, where $\bm{V}$ contains all the principal components that we are looking for.

Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\\

To project the training set onto the hyperplane, you can simply compute the matrix multiplication of the training set matrix $\bm{X}$ by the matrix $\bm{W_d}$ , defined as the matrix containing the first d principal components.\\

Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95\%). Unless, of course, you are reducing dimensionality for data visualization—in that case you will generally want to reduce the dimensionality down to 2 or 3.\\

It is possible to perform complex nonlinear projections for dimensionality reduction. This is called Kernel PCA (kPCA).  It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.

\subsubsection{LLE}

Locally Linear Embedding (LLE) is another very powerful nonlinear dimensionality reduction (NLDR) technique.  It is a Manifold Learning technique that does not rely on projections like the previous algorithms.

In a nutshell, LLE works by first measuring how each training instance linearly relates to its closest neighbors, and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly).\\

Here's how LLE works: for each training instance $\bm{x}^{(i)}$, the algorithm identifies its k closest neighbors, then tries to reconstruct $\bm{x}^{(i)}$ as a linear function of these neighbors. More specifically, it finds the weights $w_{i,j}$ such as that the squared distance between $\bm{x}^{(i)}$ and $\sum_{j=1}^m w_{i,j} \bm{x}^{(j)}$ is as small as possible, assuming $w_{i,j} = 0$ if $\bm{x}^{(j)}$ is not one of the k closest neighbors of $\bm{x}^{(i)}$

\subsubsection{Other dimensionality Reduction Techniques}

There are many other dimensionality reduction techniques, here are some of the most popular ones:
\begin{itemize}
    \item Radom Projection: Projects the data to a lower dimensional space using a random linear projection.
    \item Multidimensional Scaling (MDS): Reduces dimensionality while trying to preserve the distances between the instances.
    \item Isomap: Creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between instances
    \item t-Distributed Stochastic Neighbor Embedding (t-SNE): reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space.
    \item Linear Discriminant Analysis (LDA): it is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier.
\end{itemize}

\section{Unsupervised Learning Techniques}

the most common unsupervised learning task: dimensionality reduction. In this chapter, we will look at a few more unsupervised learning tasks and algorithms:

\begin{itemize}
    \item Clustering: the goal is to group similar instances together into clusters. This is a great tool for data analysis.
    \item Anomaly detection: the objective is to learn what “normal” data looks like, and use this to detect abnormal instances.
    \item Density estimation: this is the task of estimating the probability density function (PDF) of the random process that generated the dataset.
\end{itemize}

\subsection{Clustering}

Just like in classification, each instance gets assigned to a group. However, this is an unsupervised task.

\subsubsection{K-Means}

The K-Means algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few iterations.  Note that you have to specify the number of clusters k that the algorithm must find.\\

So how does it work? Just start by placing the centroids randomly (e.g., by picking k instances at random and using their locations as centroids). Then label the instances, update the centroids, label the instances, update the centroids, and so on until the centroids stop moving.\\

In general, it will not be easy to know how to set k, and the result might be quite bad if you set it to the wrong value. A more precise approach (but also more computationally expensive) is to use the silhouette score, which is the mean silhouette coefficient over all the instances. An instance’s silhouette coefficient is equal to (b – a) / max(a, b) where a is the mean distance to the other instances in the same cluster (it is the mean intra-cluster distance), and b is the mean nearest-cluster distance, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes b, excluding the instance’s own
cluster).

An even more informative visualization is obtained when you plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a silhouette diagram.

Limits of K-Means:\\
Despite its many merits, most notably being fast and scalable, K-Means is not perfect.  As we saw, it is necessary to run the algorithm several times to avoid sub-optimal solutions, plus you need to specify the number of clusters, which can be quite a hassle.  Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes.

\subsubsection{DBSCAN}

This algorithm defines clusters as continuous regions of high density. It is actually quite simple:

\begin{itemize}
    \item For each instance, the algorithm counts how many instances are located within a small distance $\epsilon$ from it. This region is called the instance’s $\epsilon -$ neighborhood.
    \item If an instance has at least min\_samples instances in its $\epsilon$-neighborhood (including itself), then it is considered a core instance. In other words, core instances are those that are located in dense regions.
    \item All instances in the neighborhood of a core instance belong to the same cluster.  This may include other core instances, therefore a long sequence of neighboring core instances forms a single cluster.
    \item Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.
\end{itemize}

\subsection{Gaussian Mixtures}

A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density and orientation.\\

There are several GMM variants: in the simplest variant, implemented in the Gaus sianMixture class, you must know in advance the number k of Gaussian distributions. The dataset $\bm{X}$ is assumed to have been generated through the following probabilistic process:

\begin{itemize}
    \item For each instance, a cluster is picked randomly among k clusters. The probability of choosing the $j^{th}$ cluster is defined by the cluster’s weight $\phi^{(j)}$. 7 The index of the cluster chosen for the $i^{th}$ instance is noted $z^{(i)}$
    \item If $z^{(i)} = j$, meaning the $i^{th}$ instance has been assigned to the $j^{th}$ cluster, the location
$\bm{x}^{(i)}$ of this instance is sampled randomly from the Gaussian distribution with
mean $\bm{\mu}^{(j)}$ and covariance matrix $\bm{\Sigma}^{(j)}$. This is noted $\bm{x}^{(i)} \sim \mathscr{N}(\mu^{(j)}, \Sigma^{(j)})$.
\end{itemize}

\section{Neural Network and Deep Learning}

\end{document}
